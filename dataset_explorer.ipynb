{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "from zipfile import ZipFile\n",
    "from io import BytesIO\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import seaborn as sns\n",
    "import requests\n",
    "from zipfile import ZipFile\n",
    "from io import BytesIO\n",
    "\n",
    "# Définir les URL des ensembles de données et les chemins locaux\n",
    "datasets = {\n",
    "    \"SGD\": {\n",
    "        \"url\": \"https://github.com/google-research-datasets/dstc8-schema-guided-dialogue/archive/refs/heads/master.zip\",\n",
    "        \"path\": \"data/SGD/\"\n",
    "    },\n",
    "    \"MultiWOZ\": {\n",
    "        \"url\": \"https://github.com/budzianowski/multiwoz/archive/refs/heads/master.zip\",\n",
    "        \"path\": \"data/MultiWOZ/\"\n",
    "    },\n",
    "    \"ABCD\": {\n",
    "        \"url\": \"https://github.com/asappresearch/abcd/raw/master/data/abcd_v1.1.json.gz\",\n",
    "        \"path\": \"data/ABCD/\"\n",
    "    },\n",
    "    \"BiToD\": {\n",
    "        \"url\": \"https://github.com/HLTCHKUST/BiToD/archive/refs/heads/main.zip\",\n",
    "        \"path\": \"data/BiToD/\"\n",
    "    },\n",
    "    \"SMCalFlow\": {\n",
    "        \"url\": \"https://github.com/microsoft/task_oriented_dialogue_as_dataflow_synthesis/archive/refs/heads/main.zip\",\n",
    "        \"path\": \"data/SMCalFlow/\"\n",
    "    },\n",
    "    \"TreeDST\": {\n",
    "        \"url\": \"https://github.com/microsoft/task_oriented_dialogue_as_dataflow_synthesis/archive/refs/heads/main.zip\",\n",
    "        \"path\": \"data/TreeDST/\"\n",
    "    }\n",
    "}\n",
    "\n",
    "# Fonction pour télécharger et extraire les ensembles de données\n",
    "def download_and_extract(dataset_name, dataset_info):\n",
    "    if not os.path.exists(dataset_info[\"path\"]):\n",
    "        os.makedirs(dataset_info[\"path\"])\n",
    "        print(f\"Téléchargement de {dataset_name}...\")\n",
    "        response = requests.get(dataset_info[\"url\"])\n",
    "        with ZipFile(BytesIO(response.content)) as zip_ref:\n",
    "            zip_ref.extractall(dataset_info[\"path\"])\n",
    "        print(f\"{dataset_name} téléchargé et extrait.\")\n",
    "    else:\n",
    "        print(f\"{dataset_name} existe déjà. Téléchargement ignoré.\")\n",
    "\n",
    "# Télécharger les ensembles de données\n",
    "for name, info in datasets.items():\n",
    "    download_and_extract(name, info)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gzip\n",
    "import shutil\n",
    "from zipfile import ZipFile\n",
    "\n",
    "def check_and_unzip(directory):\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            file_path = os.path.join(root, file)\n",
    "            # Handling .zip files\n",
    "            if file.endswith('.zip'):\n",
    "                extract_path = os.path.splitext(file_path)[0]\n",
    "                if not os.path.exists(extract_path):\n",
    "                    print(f\"Extraction de {file_path}...\")\n",
    "                    with ZipFile(file_path, 'r') as zip_ref:\n",
    "                        zip_ref.extractall(extract_path)\n",
    "                    print(f\"{file_path} extrait.\")\n",
    "                else:\n",
    "                    print(f\"{extract_path} existe déjà. Extraction ignorée.\")\n",
    "            # Handling .gz files\n",
    "            elif file.endswith('.gz'):\n",
    "                extract_path = os.path.splitext(file_path)[0]  # Remove the .gz extension\n",
    "                if not os.path.exists(extract_path):\n",
    "                    print(f\"Extraction de {file_path}...\")\n",
    "                    with gzip.open(file_path, 'rb') as f_in:\n",
    "                        with open(extract_path, 'wb') as f_out:\n",
    "                            shutil.copyfileobj(f_in, f_out)\n",
    "                    print(f\"{file_path} extrait.\")\n",
    "                else:\n",
    "                    print(f\"{extract_path} existe déjà. Extraction ignorée.\")\n",
    "\n",
    "# Call the function with your directory\n",
    "check_and_unzip('data/ABCD')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def get_folder_size(directory):\n",
    "    total_size = 0\n",
    "    for dirpath, dirnames, filenames in os.walk(directory):\n",
    "        for filename in filenames:\n",
    "            filepath = os.path.join(dirpath, filename)\n",
    "            total_size += os.path.getsize(filepath)\n",
    "    return total_size\n",
    "\n",
    "def print_folder_sizes(base_directory):\n",
    "    for root, dirs, files in os.walk(base_directory):\n",
    "        for dir_name in dirs:\n",
    "            dir_path = os.path.join(root, dir_name)\n",
    "            size = get_folder_size(dir_path)\n",
    "            print(f\"Folder: {dir_path}, Size: {size / (1024 ** 2):.2f} MB\")  # Convert bytes to MB\n",
    "\n",
    "# Change 'data' to your base folder name if needed\n",
    "print_folder_sizes('./data')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Exploring SGD...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'json' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 81\u001b[0m\n\u001b[0;32m     77\u001b[0m     \u001b[38;5;66;03m# Additional analyses can be added here (e.g., slot distributions, confusion matrices)\u001b[39;00m\n\u001b[0;32m     78\u001b[0m \n\u001b[0;32m     79\u001b[0m \u001b[38;5;66;03m# Explore each dataset\u001b[39;00m\n\u001b[0;32m     80\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, info \u001b[38;5;129;01min\u001b[39;00m datasets\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m---> 81\u001b[0m     \u001b[43mexplore_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minfo\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpath\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[3], line 43\u001b[0m, in \u001b[0;36mexplore_dataset\u001b[1;34m(dataset_name, dataset_path)\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mExploring \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     42\u001b[0m \u001b[38;5;66;03m# Load data\u001b[39;00m\n\u001b[1;32m---> 43\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mload_json_files\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;66;03m# Example: Analyze dialogue lengths\u001b[39;00m\n\u001b[0;32m     46\u001b[0m dialogue_lengths \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mlen\u001b[39m(dialogue[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mturns\u001b[39m\u001b[38;5;124m'\u001b[39m]) \u001b[38;5;28;01mfor\u001b[39;00m dialogue \u001b[38;5;129;01min\u001b[39;00m data]\n",
      "Cell \u001b[1;32mIn[3], line 35\u001b[0m, in \u001b[0;36mload_json_files\u001b[1;34m(directory)\u001b[0m\n\u001b[0;32m     33\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m file\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.json\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m     34\u001b[0m             \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(root, file), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m---> 35\u001b[0m                 data\u001b[38;5;241m.\u001b[39mappend(\u001b[43mjson\u001b[49m\u001b[38;5;241m.\u001b[39mload(f))\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "\u001b[1;31mNameError\u001b[0m: name 'json' is not defined"
     ]
    }
   ],
   "source": [
    "datasets = {\n",
    "    \"SGD\": {\n",
    "        \"url\": \"https://github.com/google-research-datasets/dstc8-schema-guided-dialogue/archive/refs/heads/master.zip\",\n",
    "        \"path\": \"data/SGD/\"\n",
    "    },\n",
    "    \"MultiWOZ\": {\n",
    "        \"url\": \"https://github.com/budzianowski/multiwoz/archive/refs/heads/master.zip\",\n",
    "        \"path\": \"data/MultiWOZ/\"\n",
    "    },\n",
    "    \"ABCD\": {\n",
    "        \"url\": \"https://github.com/asappresearch/abcd/raw/master/data/abcd_v1.1.json.gz\",\n",
    "        \"path\": \"data/ABCD/\"\n",
    "    },\n",
    "    \"BiToD\": {\n",
    "        \"url\": \"https://github.com/HLTCHKUST/BiToD/archive/refs/heads/main.zip\",\n",
    "        \"path\": \"data/BiToD/\"\n",
    "    },\n",
    "    \"SMCalFlow\": {\n",
    "        \"url\": \"https://github.com/microsoft/task_oriented_dialogue_as_dataflow_synthesis/archive/refs/heads/main.zip\",\n",
    "        \"path\": \"data/SMCalFlow/\"\n",
    "    },\n",
    "    \"TreeDST\": {\n",
    "        \"url\": \"https://github.com/microsoft/task_oriented_dialogue_as_dataflow_synthesis/archive/refs/heads/main.zip\",\n",
    "        \"path\": \"data/TreeDST/\"\n",
    "    }\n",
    "}\n",
    "\n",
    "# Function to load JSON files from a directory\n",
    "def load_json_files(directory):\n",
    "    data = []\n",
    "    for root, _, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            if file.endswith(\".json\"):\n",
    "                with open(os.path.join(root, file), 'r', encoding='utf-8') as f:\n",
    "                    data.append(json.load(f))\n",
    "    return data\n",
    "\n",
    "# Function to explore dataset\n",
    "def explore_dataset(dataset_name, dataset_path):\n",
    "    print(f\"\\nExploring {dataset_name}...\")\n",
    "\n",
    "    # Load data\n",
    "    data = load_json_files(dataset_path)\n",
    "\n",
    "    # Example: Analyze dialogue lengths\n",
    "    dialogue_lengths = [len(dialogue['turns']) for dialogue in data]\n",
    "    print(f\"Average dialogue length: {sum(dialogue_lengths) / len(dialogue_lengths):.2f} turns\")\n",
    "\n",
    "    # Plot dialogue length distribution\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.histplot(dialogue_lengths, bins=30, kde=True)\n",
    "    plt.title(f\"{dataset_name} Dialogue Length Distribution\")\n",
    "    plt.xlabel(\"Number of Turns\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.show()\n",
    "\n",
    "    # Example: Analyze intents (if available)\n",
    "    intents = []\n",
    "    for dialogue in data:\n",
    "        for turn in dialogue['turns']:\n",
    "            if 'intent' in turn:\n",
    "                intents.append(turn['intent'])\n",
    "\n",
    "    if intents:\n",
    "        intent_counts = Counter(intents)\n",
    "        print(f\"Top 5 intents in {dataset_name}: {intent_counts.most_common(5)}\")\n",
    "\n",
    "        # Plot intent distribution\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        sns.barplot(x=[count for _, count in intent_counts.most_common(10)],\n",
    "                    y=[intent for intent, _ in intent_counts.most_common(10)])\n",
    "        plt.title(f\"Top 10 Intents in {dataset_name}\")\n",
    "        plt.xlabel(\"Frequency\")\n",
    "        plt.ylabel(\"Intent\")\n",
    "        plt.show()\n",
    "\n",
    "    # Additional analyses can be added here (e.g., slot distributions, confusion matrices)\n",
    "\n",
    "# Explore each dataset\n",
    "for name, info in datasets.items():\n",
    "    explore_dataset(name, info[\"path\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venvForContinuousLearningThesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
